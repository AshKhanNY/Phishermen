{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMGWxQCO7s0e",
        "outputId": "4c96ea94-a0bc-40b4-c8e3-a3f9dbdbbebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.8/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.8/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch==1.8.0 torchtext==0.9.0\n",
        "\n",
        "# Reload environment\n",
        "exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1C2TexgvPn8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_files\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import urllib.request as ur\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "#Label = pickle.loads(f)\n",
        "\n",
        "#with open(\"FinalLink\", \"rb\") as fp:   # Unpickling\n",
        "\n",
        "#  ValidLink = pickle.load(fp)\n",
        "\n",
        "with open(\"Fianlcontents\", \"rb\") as fp:   # Unpickling\n",
        "\n",
        "  contents = pickle.load(fp)\n",
        "\n",
        "with open(\"FinalLabel\", \"rb\") as fp:   # Unpickling\n",
        "\n",
        "  Label = pickle.load(fp)\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd  \n",
        "  \n",
        "     \n",
        "# list of name, degree, score \n",
        "text = [\"text\", \"labels\"] \n",
        "label = [0,1] \n",
        "\n",
        "     \n",
        "# dictionary of lists  \n",
        "dict = {'text':contents, 'labels':Label}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "# saving the dataframe \n",
        "df.to_csv('Finaltext.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTOzBVdFvU7j"
      },
      "outputs": [],
      "source": [
        "#test = pd.read_csv(\"web.csv\")\n",
        "train = pd.read_csv(\"Finaltext.csv\")\n",
        "\n",
        "train.head\n",
        "#print(train.target)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(contents,Label,test_size=0.35,shuffle=True)\n",
        "\n",
        "dict = {'text':list(x_train), 'labels':list(y_train)}  \n",
        "  \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "# saving the dataframe \n",
        "df=df.to_csv('train.csv')\n",
        "\n",
        "\n",
        "\n",
        "dict = {'text':list(x_test), 'labels':list(y_test)}  \n",
        "  \n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "# saving the dataframe \n",
        "df=df.to_csv('test.csv')\n",
        "\n",
        "\n",
        "\n",
        "train=pd.read_csv('train.csv')\n",
        "\n",
        "test=pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rNYySDDMsU2"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db83NX_PvYau"
      },
      "outputs": [],
      "source": [
        "# drop 'id' , 'keyword' and 'location' columns.\n",
        "#train.drop(columns=['id','keyword','location'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsFlNh4YvbJ9"
      },
      "outputs": [],
      "source": [
        "# to clean data\n",
        "def removeDigits(List):\n",
        "    \n",
        "    answer = []\n",
        "    for s in List:\n",
        "      res=\"\"\n",
        "      for char in s :\n",
        "        if not char.isdigit() and char!=' ':\n",
        "              res+=char\n",
        "      if len(res)>=2:\n",
        "        answer.append(res)\n",
        "    return ' '.join(answer)\n",
        "\n",
        "\n",
        "def normalise_text (text):\n",
        "    text = text.str.lower() # lowercase\n",
        "    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n",
        "    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
        "    text = text.str.replace(r\"@\",\"\")\n",
        "    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n",
        "    text = text.str.replace(\"\\s{2,}\", \" \")\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srreoP6ovc-Y"
      },
      "outputs": [],
      "source": [
        "train[\"text\"]=normalise_text(train[\"text\"])\n",
        "\n",
        "test[\"text\"]=normalise_text(test[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMEilQxYvfh7"
      },
      "outputs": [],
      "source": [
        "# split data into train and validation \n",
        "train_df, valid_df = train_test_split(train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBmpSBCzvhMK"
      },
      "outputs": [],
      "source": [
        "SEED = 2022\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTQK6x0UviiX"
      },
      "outputs": [],
      "source": [
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "print(LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLT0fWpovjFP"
      },
      "outputs": [],
      "source": [
        "class DataFrameDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, df, fields, is_test=False, **kwargs):\n",
        "        examples = []\n",
        "        for i, row in df.iterrows():\n",
        "            label = row.labels if not is_test else None\n",
        "            text = row.text\n",
        "            examples.append(data.Example.fromlist([text, label], fields))\n",
        "\n",
        "        super().__init__(examples, fields, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return len(ex.text)\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n",
        "        train_data, val_data, test_data = (None, None, None)\n",
        "        data_field = fields\n",
        "\n",
        "        if train_df is not None:\n",
        "            train_data = cls(train_df.copy(), data_field, **kwargs)\n",
        "        if val_df is not None:\n",
        "            val_data = cls(val_df.copy(), data_field, **kwargs)\n",
        "        if test_df is not None:\n",
        "            test_data = cls(test_df.copy(), data_field, True, **kwargs)\n",
        "\n",
        "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgF-swAZvm9L"
      },
      "outputs": [],
      "source": [
        "fields = [('text',TEXT), ('label',LABEL)]\n",
        "\n",
        "train_ds, val_ds = DataFrameDataset.splits(fields, train_df=train_df, val_df=valid_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s0L-y71vqL7"
      },
      "outputs": [],
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT.build_vocab(train_ds, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = 'glove.6B.200d',\n",
        "                 unk_init = torch.Tensor.zero_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O_vz2ICvqS8"
      },
      "outputs": [],
      "source": [
        " \n",
        "LABEL.build_vocab(train_ds)\n",
        "\n",
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "print(TEXT.vocab.stoi) \n",
        "\n",
        "\n",
        "# load pickle module\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "# create a binary pickle file \n",
        "f = open(\"file.pkl\",\"wb\")\n",
        "\n",
        "# write the python object (dict) to pickle file\n",
        "pickle.dump(TEXT.vocab.stoi,f)\n",
        "\n",
        "# close file\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QIEej8AvvRd"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (train_ds, val_ds), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGoJN9Fsvw9y"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 15\n",
        "learning_rate = 0.001\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.2\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVSh6gsL9Y1r"
      },
      "outputs": [],
      "source": [
        "class LSTM_net(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        \n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [sent len, batch size]\n",
        "\n",
        "        print(text.shape)\n",
        "\n",
        "        print(text_lengths.shape)\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        # embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        output = self.fc1(hidden)\n",
        "        output = self.dropout(self.fc2(output))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlNVDyLWQg_w"
      },
      "outputs": [],
      "source": [
        "model = LSTM_net(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdLRlIScv2dW"
      },
      "outputs": [],
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G_AYe1lv4Ja"
      },
      "outputs": [],
      "source": [
        "#  to initiaise padded to zeros\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2QWheXhv5rl"
      },
      "outputs": [],
      "source": [
        "model.to(device) #CNN to GPU\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY7aELYHv7Ng"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    print(rounded_preds)\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4seH_mrIv-VP"
      },
      "outputs": [],
      "source": [
        "# training function \n",
        "def train(model, iterator):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        text, text_lengths = batch.text    \n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMJ1YE7vv-1h"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, iterator):\n",
        "    count=0    \n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            \n",
        "            text, text_lengths = batch.text\n",
        "            print(text.shape,text_lengths.shape)\n",
        "              \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            \n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6inE7272yGS"
      },
      "outputs": [],
      "source": [
        "t = time.time()\n",
        "loss=[]\n",
        "acc=[]\n",
        "val_acc=[]\n",
        "best_train_loss = float('inf')\n",
        "for epoch in range(15):\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator)\n",
        "    \n",
        "    valid_acc = evaluate(model, valid_iterator)\n",
        "    if train_loss < best_train_loss:\n",
        "        best_train_loss = train_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n",
        "    \n",
        "    loss.append(train_loss)\n",
        "    acc.append(train_acc)\n",
        "    val_acc.append(valid_acc)\n",
        "    \n",
        "print(f'time:{time.time()-t:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbVIZjMtaC7B"
      },
      "outputs": [],
      "source": [
        "path=\"saved_weights.pt\"\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "\n",
        "#inference \n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def predict(model, sentence):\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence \n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n",
        "    length = [len(indexed)]                                    #compute no. of words\n",
        "    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n",
        "    tensor = tensor.unsqueeze(1)                             #reshape in form of batch,no. of words\n",
        "    length_tensor = torch.LongTensor(length)                   #convert to tensor\n",
        "    prediction = model(tensor, length_tensor)\n",
        "    prediction=torch.sigmoid(prediction)        \n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuV0w0YbgvX5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_files\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import urllib.request as ur\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "def removeDigits(List):\n",
        "    \n",
        "    answer = []\n",
        "    for s in List:\n",
        "      res=\"\"\n",
        "      for char in s :\n",
        "        if not char.isdigit() and char!=' ':\n",
        "              res+=char\n",
        "      if len(res)>=2:\n",
        "        answer.append(res)\n",
        "    return ' '.join(answer)\n",
        "#Fetch the contents from website and rmove all the unessary character,punctuations\n",
        "def fetch_content(url):\n",
        "\n",
        "      if \"http://\" and \"https://\" not in url:\n",
        "        url=\"http://\"+url\n",
        "      X=[]\n",
        "\n",
        "      try:\n",
        "        page = requests.get(url)\n",
        "\n",
        "        soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "        # kill all script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.extract()    # rip it out\n",
        "        # get text\n",
        "        text = soup.get_text()\n",
        "        text=re.sub('[^A-Za-z0-9]+', ' ', text).strip()\n",
        "        text=str(text).replace('\\n',' ').split(' ')\n",
        "\n",
        "        for i in text:\n",
        "          if i:\n",
        "            X.append(i)\n",
        "      except:\n",
        "        X=[]\n",
        "\n",
        "      documents = []\n",
        "      stemmer = WordNetLemmatizer()\n",
        "      print(X)\n",
        "      if X:\n",
        "        #Remove the digits from string:\n",
        "        document = removeDigits(X)\n",
        "\n",
        "        # Remove all the special characters\n",
        "        document = re.sub(r'\\W', ' ', document)\n",
        "\n",
        "        # remove all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Remove single characters from the start\n",
        "        document = re.sub(r'^[a-zA-Z]\\s+', ' ', document) \n",
        "\n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "        # Removing prefixed 'b'\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "        # Converting to Lowercase\n",
        "        document = document.lower()\n",
        "\n",
        "        # Lemmatization\n",
        "        document = document.split()\n",
        "\n",
        "        document = [stemmer.lemmatize(word) for word in document]\n",
        "        document = ' '.join(document)\n",
        "\n",
        "        return document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GWZ1L1q9w3W"
      },
      "outputs": [],
      "source": [
        "content=fetch_content(\"https://phrma.org/states?utm_campaign=2022-q4-pbm-afd&utm_medium=pai_srh_cpc-ggl-adf&utm_source=ggl&utm_content=clk-pat-tpv_scl-geo_std-usa-nj-pai_srh_cpc-ggl-adf-NewJerseyStatePBMsSearchWC2-prc_brr-edu-inf-lrm-soc_txt-std-vrb-art&utm_term=&https://p2a.co/pqccffz&gclid=EAIaIQobChMIhJy5kPP1-wIVGMzICh02VwyAEAAYASAAEgJvaPD_BwE\")\n",
        "\n",
        "print(content)\n",
        "\n",
        "\n",
        "predict(model, content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eokuO3r4HZVw"
      },
      "outputs": [],
      "source": [
        "for batch in train_iterator:\n",
        "    \n",
        "    print(batch.text[0].shape)\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}